â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Cluster Autoscaling with GPU Accelerators
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Test Started: 2025-10-28 12:10:40 UTC
Description: Validates that cluster autoscaler scales GPU node groups based on pending GPU workloads
Primary Namespace: cluster-autoscaling


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Requirement Specification
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

MUST: If the platform provides a cluster autoscaler or an equivalent mechanism, it must be able to scale up/down node groups containing specific accelerator types based on pending pods requesting those accelerators.

How we might test it: Prepare a node pool with N nodes, configured with a specific accelerator type, with min node pool size of N and max size of at least N+1. Assuming 1 accelerator A per node N, Create (A*N)+1 Pods, each requesting one accelerator resource from that pool, verify that at least one Pod is unschedulable (Pending), and the cluster autoscaler will increase the node count to N+1, causing the Pod to be Running. Delete that Pod, then the cluster autoscaler will remove the idle accelerator node, returning the node count to N.

Note: It is not allowed to "cheat" and require certain affinities. The cluster autoscaler **must** react on pods that **request** accelerators.


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Pre-Test Cleanup Check
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… No leftover resources found

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Checking Kubernetes Access
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… kubectl is available
âœ… Connected to Kubernetes cluster

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Step 1: Verify GPU Infrastructure
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â„¹ï¸ Checking for GPU nodes...
âœ… Found 1 GPU node(s) - first node: ip-10-180-14-93.eu-central-1.compute.internal
â„¹ï¸ Checking GPU device plugin...
âœ… NVIDIA GPU device plugin is running
â„¹ï¸ Initial GPU allocation: 0/1

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Step 2: Create Test Namespace
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â„¹ï¸ Creating namespace: cluster-autoscaling
âœ… Namespace created: cluster-autoscaling
âœ… Test namespace created

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Step 3: Record Initial Cluster State
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â„¹ï¸ Initial GPU node count: 1

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Step 4: Deploy GPU Workload Requiring Scale-Up
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â„¹ï¸ Creating deployment with 2 pods requiring 1 GPU each (exceeds single node capacity)
â„¹ï¸ Important: No nodeAffinity used - autoscaler must react to GPU requests only
deployment.apps/gpu-test-workload created
âœ… GPU workload deployment created (2 replicas Ã— 1 GPU each)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Step 5: Wait for Initial Pod Scheduling
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â„¹ï¸ Current pod state: 1 running, 1 pending
âœ… Expected pod distribution: 1 running (node capacity reached), 1 pending (needs additional node)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Step 6: Wait for Cluster Autoscaler Scale-Up
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â„¹ï¸ Waiting for GPU node count to reach 2 (timeout: 900s)
â„¹ï¸ Current: 1, Expected: 2, Elapsed: 1s
â„¹ï¸ Current: 1, Expected: 2, Elapsed: 16s
â„¹ï¸ Current: 1, Expected: 2, Elapsed: 31s
â„¹ï¸ Current: 1, Expected: 2, Elapsed: 46s
â„¹ï¸ Current: 1, Expected: 2, Elapsed: 61s
âœ… GPU node count reached 2 after 76s
âœ… Cluster autoscaler successfully scaled up to 2 GPU nodes
â„¹ï¸ Post-scale GPU node count: 2
â„¹ï¸ Post-scale GPU allocation: 0/1

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Step 7: Wait for GPU Device Plugin on All Nodes
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â„¹ï¸ Waiting for GPU device plugin to initialize on all nodes...
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 1
â„¹ï¸ GPU nodes: 2, Device plugin pods running: 2
âœ… GPU device plugin running on all GPU nodes
â„¹ï¸ Waiting for GPU resources to be advertised on new nodes...
â„¹ï¸ Nodes advertising GPU resources: 2

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Step 8: Verify All Pods Are Scheduled
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â„¹ï¸ Current pod state: 2 running, 0 pending
âœ… All pods successfully scheduled on GPU nodes

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Step 9: Test Scale Down
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â„¹ï¸ Deleting GPU workload to trigger scale down...
deployment.apps "gpu-test-workload" deleted
â„¹ï¸ Remaining pods in namespace: 0
â„¹ï¸ Waiting for cluster autoscaler to scale down (may take up to 20 minutes)...
â„¹ï¸ Waiting for GPU node count to reach 1 (timeout: 1200s)
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 0s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 15s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 31s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 46s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 61s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 76s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 91s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 106s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 122s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 137s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 152s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 167s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 182s
â„¹ï¸ Current: 2, Expected: 1, Elapsed: 197s
âœ… GPU node count reached 1 after 213s
âœ… Cluster autoscaler successfully scaled down to 1 GPU node
â„¹ï¸ Final GPU node count: 1
â„¹ï¸ Final GPU allocation: 0/1

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Test Summary
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Test Results:
  Initial GPU Nodes: 1
  Scale Up Target: 2 nodes
  Post-Scale Nodes: 2
  Final Running Pods: 2
  Scale Down Target: 1 node
  Final Nodes: 1

CONFORMANCE STATUS: âœ… PASSED

The cluster autoscaler successfully:
  âœ… Scaled up GPU nodes when workloads exceeded capacity
  âœ… Pending pods requesting GPUs triggered scale-up correctly
  âœ… All GPU pods were scheduled after scale-up completed
  âœ… Scaled down GPU nodes when workloads were removed

The platform meets the cluster autoscaling requirement for GPU accelerator workloads.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Test Result
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Cluster autoscaler correctly scaled GPU nodes based on pending GPU workloads

ğŸ‰ Test completed successfully!


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ Final Cleanup
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â„¹ï¸ Test completed. Cleaning up...
â„¹ï¸ Deleting namespace: cluster-autoscaling
  namespace "cluster-autoscaling" deleted
âœ… Cleanup completed
